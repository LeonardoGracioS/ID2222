{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark import sql\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "sqlContext = sql.SQLContext(sc)\n",
    "\n",
    "file_location = \"./0\"\n",
    "df = sqlContext.read.csv(file_location, header=True, mode=\"DROPMALFORMED\")\n",
    "df = df.dropDuplicates([\"path\"])\n",
    "table = df.filter(~ df.path.contains(\".html\")).filter(df.path.isNotNull()).filter(df.Abstract.isNotNull()).cache()\n",
    "\n",
    "n_el = 20\n",
    "subset = table.limit(n_el)\n",
    "testDf = sc.parallelize([Row(\"test.txt\", \"\", \"The principal investigator will develop and improve methods  for understanding the stability of nonlinear waves as solutions  of partial differential equations which arise in a number of  scientific fields. In one particular case, he will use ideas he  has recently developed to establish the exponential decay rate  for nonlinear perturbations in solitary wave problems for KdV-  type equations.  The qualitative analysis of nonlinear waves for such systems  is of significant scientific interest in view of the wide spread  occurrence of such phenomena in real world situations.\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\")]).toDF()\n",
    "subset = subset.union(testDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "def shingle(text, shingle_length):\n",
    "    shingles = set(text[i:i+shingle_length] for i in range(len(text)-shingle_length+1))\n",
    "    return shingles\n",
    "\n",
    "class Shingling:\n",
    "    def __init__(self, rdd, k = 10):\n",
    "        self.rdd = rdd\n",
    "        self.shingle_length = k\n",
    "    \n",
    "    def is_prime(self, x):\n",
    "        for i in range(3, int(math.sqrt(x)) + 1, 2):\n",
    "            if x % i == 0:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    \n",
    "    def generate_prime(self, treshold):\n",
    "        if treshold == 2 or treshold == 1:\n",
    "            return treshold+1\n",
    "        \n",
    "        if treshold%2 == 0:\n",
    "            treshold+=1\n",
    "        \n",
    "        while not self.is_prime(treshold):\n",
    "            treshold +=2\n",
    "        \n",
    "        return treshold\n",
    "    \n",
    "    def generate_random_vals(self, n_func, tot_shingles):#tot_shingles = maximum shingle value\n",
    "        '''\n",
    "        * The permutations are represented by randomized hash functions: ax + b % p.\n",
    "        * p is a prime such that p >= n where n is the number of terms in the collection.\n",
    "        * If this constant isn’t prime, the random hash produces a lot of collisions, and the algorithm doesn’t work well\n",
    "        * a and b are chosen uniformly at random from {1,2,...,p-1} (indip.).\n",
    "        '''\n",
    "        self.a = random.sample(range(1, tot_shingles), n_func)\n",
    "        self.b = random.sample(range(1, tot_shingles), n_func)\n",
    "        self.c = self.generate_prime(tot_shingles)\n",
    "    \n",
    "    def multiple_shingle(self, field = \"Abstract\", merged = False):#shingle a text\n",
    "        l = self.shingle_length # avoid references to \"self\" in the spark command, otherwise spark will serialize the class\n",
    "    #(https://stackoverflow.com/questions/46178161/not-able-to-access-class-methods-from-pyspark-rdds-map-method) \n",
    "    \n",
    "        shingles = self.rdd.select(\"path\",field).rdd.flatMap(lambda r: ((shingle, r[\"path\"]) for shingle in shingle(r[field], l))).cache()#(shingle, document id)\n",
    "        shingles_set = shingles.map(lambda x: x[0]).distinct().zipWithIndex()#hashed set of shingles (text, hash)\n",
    "        if merged:\n",
    "            result = shingles_set.join(shingles).map(lambda x: (x[1][1], set(x[1][0])))#(text, (hash, document_id)) -> (document_id, {hash})\n",
    "            result = result.reduceByKey(lambda x,y: x.union(y)) #document_id, {hashes of shingles}\n",
    "        else:\n",
    "            result = shingles_set.join(shingles).map(lambda x: (x[1][1], x[1][0]))#returns [(document_id, hash),...]\n",
    "        self.shingle_rdd = result\n",
    "        return result\n",
    "    \n",
    "    def compare_docs(self, rdd1, rdd2, merged = False):#if merged : (id_doc, [set of shingle ids]), otherwise [(document_id, hash),...]\n",
    "        reverse_rdd = self.shingle_rdd.filter(lambda x:x[0] == rdd1).union(self.shingle_rdd.filter(lambda x:x[0] == rdd2))#rdd with 2 docs hashes\n",
    "        if (merged):\n",
    "            reverse_rdd = reverse_rdd.flatMap(lambda x: ((i, 1) for i in x[1])).cache() #(hash, 1)\n",
    "        else:\n",
    "            reverse_rdd = reverse_rdd.map(lambda x: (x[1], 1)).cache() #(hash, 1)\n",
    "        n_hashes = reverse_rdd.map(lambda x: x[0]).distinct().count()\n",
    "        common_items = reverse_rdd.reduceByKey(lambda x,y: x+y).map(lambda x: x[1]).filter(lambda x: x == 2).count()#if same hash twice = common hash\n",
    "        return float(common_items)/float(n_hashes)\n",
    "    \n",
    "    def set_max_index(self, n_func, merged = False):#(key, set_of_values)\n",
    "        if merged:\n",
    "            val = self.shingle_rdd.map(lambda x: max(x[1])).max()+1\n",
    "        else:\n",
    "            val = self.shingle_rdd.count()#map(lambda x: x[1]).max\n",
    "        self.generate_random_vals(n_func, val)\n",
    "        \n",
    "    def min_hashing(self, n_func, merged = False):\n",
    "        self.signature_length = n_func\n",
    "        self.set_max_index(n_func, merged)\n",
    "        if merged:\n",
    "            rdd = self.shingle_rdd.flatMap(lambda x: (x[0], v) for v in v[1])\n",
    "        a=self.a\n",
    "        b=self.b\n",
    "        c=self.c\n",
    "        self.minhash = self.shingle_rdd.flatMap(lambda x: [((x[0],i), ((a[i]*x[1])+b[i])%c) for i in range(0, len(a))]).reduceByKey(lambda x,y: min(x,y))#((doc id,sign id), val), ...\n",
    "        return self.minhash\n",
    "      \n",
    "    def compare_signatures(self, rdd1, rdd2):\n",
    "        rdd = self.minhash.filter(lambda x:x[0][0] == rdd1).union(self.minhash.filter(lambda x:x[0][0] == rdd2)).map(lambda x: (x[0][1], x[1]))#(func, value)\n",
    "        rdd = rdd.reduceByKey(lambda x,y: 1 if x==y else 0).cache()#same sign id, same value\n",
    "        common = rdd.filter(lambda x: x[1] == 1).count()\n",
    "        total = rdd.map(lambda x:x[0]).distinct().count()\n",
    "        return float(common)/float(total)\n",
    "    \n",
    "    def spark_lsh(self, band_size):\n",
    "        threshold = (1/band_size)**(band_size/self.signature_length)\n",
    "        print(\"threshold: \", threshold)\n",
    "        global_rdd = self.minhash.map(lambda x: ((x[0][0], x[0][1]//band_size), {x[0][1]%band_size : x[1]})).reduceByKey(lambda x,y: {**x, **y})\n",
    "        global_rdd = global_rdd.map(lambda x:(x[0], hash(tuple([x[1][i] for i in range(0, len(x[1]))])))).map(lambda x: ((x[0][1],x[1]), x[0][0]))\n",
    "        global_rdd = global_rdd.groupByKey().flatMap(lambda x: ((frozenset(i), 1) for i in itertools.combinations(x[1], 2)))\n",
    "        global_rdd = global_rdd.reduceByKey(lambda x,y: x+y).filter(lambda x: x[1] >= threshold).collect()\n",
    "        return global_rdd\n",
    "                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exac 1.0\n",
      "approx 1.0\n",
      "threshold:  0.251188643150958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(frozenset({'dbfs:/data-mining/nsf-abstract/original/awd_1992_01/a9201869.txt',\n",
       "             'test.txt'}),\n",
       "  5)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shingling = Shingling(subset, 8)\n",
    "s = shingling.multiple_shingle()\n",
    "c = shingling.compare_docs(\"test.txt\",\"dbfs:/data-mining/nsf-abstract/original/awd_1992_01/a9201869.txt\")\n",
    "\n",
    "l_sign = 5000\n",
    "band_size = l_sign/5\n",
    "\n",
    "d = shingling.min_hashing(l_sign)\n",
    "e = shingling.compare_signatures(\"test.txt\", \"dbfs:/data-mining/nsf-abstract/original/awd_1992_01/a9201869.txt\")\n",
    "\n",
    "print(\"exac\", c)\n",
    "print(\"approx\", e)\n",
    "\n",
    "shingling.spark_lsh(band_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "import numpy as np \n",
    "import itertools\n",
    "\n",
    "def lsh(M,b,r): #M = matrice of signatures, b = nb of bands, r = nb of rows\n",
    "  \n",
    "  \n",
    "\n",
    "  \n",
    "  nrows, ncols = np.shape(M)\n",
    "  \n",
    "  if b*r != nrows:\n",
    "    print('Error branding')\n",
    "    \n",
    "  # computing the threshold\n",
    "  threshold = (1/b)**(1/r)\n",
    "  \n",
    "  # the different bands\n",
    "  bands = np.array([M[i:i+r, :] for i in range(b)])\n",
    "  \n",
    "  #print(bands)\n",
    "\n",
    "  #initialisation\n",
    "  all_candidate_pairs=[]\n",
    "  buckets = []\n",
    "  \n",
    "  for i in range(len(bands)): \n",
    "    # initialisation \n",
    "    band =bands[i]\n",
    "    candidate_pairs_band =[] # the list of candidate pairs according to this band\n",
    "    \n",
    "    # the different portions : r-vectors of the band\n",
    "    portions = np.array([band[:,j] for j in range(ncols)])\n",
    "    \n",
    "    print(portions)\n",
    "    # a bucket for this band that hashes every portion\n",
    "    bucket_array = np.array([hashing(portion) for portion in portions]) \n",
    "    \n",
    "    # go through the different unique values of hashes and find the similar vectors that hashed to this value\n",
    "    for hash_value in np.unique(bucket_array):\n",
    "      \n",
    "      indexes_of_portions_with_same_hash = np.where(bucket_array == hash_value)[0]\n",
    "      candidate_pairs_value = list(itertools.combinations(indexes_of_portions_with_same_hash,2))\n",
    "\n",
    "      if (candidate_pairs_value != []):\n",
    "        candidate_pairs_band +=(candidate_pairs_value)\n",
    "        all_candidate_pairs += (candidate_pairs_value)\n",
    "        \n",
    "    # Different buckets to different bands\n",
    "    buckets.append(bucket_array)\n",
    "  \n",
    "  all_candidate_pairs = np.unique(all_candidate_pairs,return_counts=True)\n",
    "  return(all_candidate_pairs, threshold)\n",
    "\n",
    "\n",
    "def hashing(array): # computes the hash of a portion to a fixed range of values, with k > nb of documents and large\n",
    "  hash_out = hash(tuple(array))\n",
    "  return hash_out\n",
    "\n",
    "def check(M,all_candidate_pairs, t): # Check each candidate pair’s signatures if the fraction of  components in which they agree is at least t , i.e. if they  are similar at least  t\n",
    "  \n",
    "  nrows, ncols = np.shape(M)\n",
    "  print ('the threshold is ', t)\n",
    "  similar_pairs = []\n",
    "  \n",
    "  for pair in all_candidate_pairs[0]:\n",
    "    \n",
    "    similarity = compareSignatures(M[:,pair[0]],M[:,pair[1]])\n",
    "    \n",
    "    if similarity >= t :\n",
    "      similar_pairs.append([tuple(pair), similarity])\n",
    "      print('the pair ', pair, ' has similar signatures ', similarity)\n",
    "      \n",
    "    if similarity <t:\n",
    "      print('the pair ', pair,\" is not similar enough\")\n",
    "      \n",
    "      \n",
    "      \n",
    "  return similar_pairs\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "main",
  "notebookId": 3961307530736504
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
